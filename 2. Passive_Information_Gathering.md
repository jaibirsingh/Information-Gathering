# Passive Information Gathering

This type of information is basically gathered through publicly available sources, or prominently without actively engaging with the target.

***
Generally, following entities are gathered while performing Passive reconnaissance:
1. IP addresses 
2. DNS information - Domain names and ownership info.
3. E-mail addresses and social media profiles
4. Web technology used on target sites
5. Sub-domains
6. Physical addresses / Phone numbers
7. Directories hidden from the search engines

***

# COMMANDS

### host[URL]
* This command gives out the I.P. address of "URL"
* If it returns multiple IPs, most likely, a firewall/ Proxy is setup for "URL"

### whatweb[URL]
* To get more information about the website

### whois[URL]
whois is a public database that houses the information collected when someone registers a domain name or updates their DNS settings.

### dns recon -d [URL]
Website reconnaissance

### wafw00f [URL]
* WAF = Web Application Firewall
* wafw00f = Web Application Firewall Fingerprinting tool
* wafw00f [URL] -a = This will list all the WAF instances related to "URL"

### sublist3r -d [URL]
* Python tool
* It is used to enumerate sub-domains of websites using OSINT(Open Source InTelligence)
* Enumerates sub-domains using engines like Google, Yahoo, Bing, Baidu, Ask
* Enumerates using tools like: NetCraft, DNSDumpster, ThreatCrowd, VirusTotal, ReverseDNS
* SUBBRUTE is an integration with sublist3r for brut force actions which are a part of active information gathering.
  
  `sudo apt-get install sublist3r`

  `sublist3r -d [URL] -e [search engine 1],[search engine 2],....`

* For more info. about sublist3r, its github repo can be reffered.


### theHarvester
* Use it for OSINT gathering to help determine a company's external threat landscape on the internet
* The tool uses multiple public data sources.
* The tool gathers A): Names B): Emails C): IPs D): URLs E): Sub-domains
  
  ` theHarvester -d [URL] -b [source to be searched] `

* The "source to be searched" canbe found on Github repo: https://github.com/laramies/theHarvester
***

# URLs to target

### [URL]/robots.txt 
* Specify files/folders not to be indexed by search engine

### [URL]/page-sitemap.xml
* Page structure of the website
***
# ADD-ONS 
### Built-with (Firefox)
### Wappalyzer (Firefox)
***

# TOOLS
### httrack.com
* To download the whole website so that the source code can be analyzed
* To install :
    `sudo apt-get install webhttrack`
* To avoid error:
  `sudo apt update`

### Netcraft
* Tool for website footprinting
### dnsdumpster.com
* Tool for DNS reconnaissance
### Google Dorks
* Google Dorks is a search string that leverages advanced search operators to find information that isnâ€™t readily available on a particular website. Sometimes called Google Hacking.
* `site:[URL]`
* `site:[URL] inurl:[Keyword]`  OR `site:[URL] [Keyword]`  where Keyword could be: Forum/ Admin, etc.
* Enumeration via Google Dorks: `site.*:[URL]` OR `site.*:[URL] intitle:[Keyword]` OR `site.*:[URL] filetype:[Keyword]` where Keyword = PDF, ZIP, xlsx, docx, etc. and * represents sub-domain
* Note: To find the list which contains all the Google Dorkscan be found on "exploit-db.com/google-hacking-database"

### Directory Listing
* Directory listing refers to the process of a web server displaying the contents of a directory when a user navigates to a directory on a website. When a directory listing is enabled, anyone who has access to the web server can see a list of the files and directories within the website directory.
* Enabling directory listing can reveal sensitive information, such as file names, folder names, and directory structures that can be used by attackers to launch further attacks on the website or its users.
* Example, when a user types www.invicti.com/learn/ in the browser address bar without specifying a file name in the URL (such as index.html, index.php, index.htm, or default.asp), the web server processes this request, returns the index HTML file for that directory (in this case, the /learn/ directory), and the web browser displays the web page. However, if the index file did not exist and directory listing was enabled, the web server would instead return the contents of a directory, like a file manager.

* Example code -> `intitle:index of`

### Older version of the website to find vulnerabilities
* `cache:[URL]`
* Tool used to do the same is called Wayback Machine (archive.org)

### To find a directory with all the passwords 
* Some website developers do not take time to secure the files containing passwords
* These sensitive files can be accessed using following commands:
  
  `inurl:auth_user_file.txt`
  
  `inurl:password.txt`

***
